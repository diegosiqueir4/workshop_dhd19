{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prerequisite instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "\n",
    "1. [Generate the label images](#Generate-label-images)\n",
    "2. [Show orginal image and generated label](#Show-orginal-image-and-generated-label)\n",
    "3. [Split data into train and test](#Split-data-into-train-and-test)\n",
    "4. [Create a `classes.txt` file](#Create-a-classes.txt-file)\n",
    "5. [Create a config file](#Create-a-config-file)\n",
    "6. [Train](#train)\n",
    "7. [Show results on test data](#See-the-results-on-test-data)\n",
    "8. [Convert into PAGEXML](#Convert-into-PAGE-XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dh_segment.io import PAGE\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread, imsave\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_dir = './images/'\n",
    "export_label_dir = './labels/'\n",
    "os.makedirs(export_label_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filenames = glob(os.path.join(images_dir, 'page/*.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in list_filenames:\n",
    "    # parse PAGEXML file\n",
    "    page = PAGE.parse_file(filename)\n",
    "\n",
    "    # create empty mask ad^nd draw the ornament regions\n",
    "    mask = np.zeros((page.image_height, page.image_width, 3), dtype=np.uint8)\n",
    "    page.draw_text_regions(mask, color=(255,0,0), fill=True)\n",
    "\n",
    "    # save mask\n",
    "    export_filename = os.path.join(export_label_dir, os.path.basename(page.image_filename).split('.')[0] + '.png')\n",
    "    \n",
    "    # save the label image\n",
    "    imsave(export_filename, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Show orginal image and generated label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_image = imread(os.path.join('./images/', page.image_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('original image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask)\n",
    "plt.title('Generated label image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Split data into train and test\n",
    "You have two options : \n",
    "1. (recommended) you modify the already existing `train_data.csv` and `test_data.csv` (used to trained the provided model)\n",
    "2. you create your train / test data from scratch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "dhd_wk_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: modify the existing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./csv/train_data.csv', header=None, names=['images', 'labels'])\n",
    "eval_data = pd.read_csv('./csv/eval_data.csv', header=None, names=['images', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the first 5 entries\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abspath_images_dir = os.path.abspath(os.path.join(dhd_wk_dir, images_dir))\n",
    "abspath_labels_dir = os.path.abspath(os.path.join(dhd_wk_dir, export_label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['images'] = train_data.images.apply(lambda x: re.sub('\\$ROOT_IMAGE_DIR', abspath_images_dir, x))\n",
    "train_data['labels'] = train_data.labels.apply(lambda x: re.sub('\\$ROOT_LABEL_DIR', abspath_labels_dir, x))\n",
    "\n",
    "eval_data['images'] = eval_data.images.apply(lambda x: re.sub('\\$ROOT_IMAGE_DIR', abspath_images_dir, x))\n",
    "eval_data['labels'] = eval_data.labels.apply(lambda x: re.sub('\\$ROOT_LABEL_DIR', abspath_labels_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.to_csv('./my_train_data.csv', header=False, index=False, encoding='utf8')\n",
    "eval_data.to_csv('./my_eval_data.csv', header=False, index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of option 1: Jump to [next step](#Create-a-classes.txt-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: create csv data from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_csv = list()\n",
    "for filename in list_filenames:\n",
    "    basename = os.path.basename(filename).split('.')[0]\n",
    "    image_filename = os.path.abspath(os.path.join(dhd_wk_dir, images_dir, basename + '.jpg'))\n",
    "    label_filename = os.path.abspath(os.path.join(dhd_wk_dir, export_label_dir, basename + '.png'))\n",
    "    if os.path.isfile(image_filename) and os.path.isfile(label_filename):\n",
    "        data_csv.append((image_filename, label_filename))\n",
    "    else:\n",
    "        print('did not found correct image / label for filename {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle data and split in in 0.9 training / 0.1 eval\n",
    "shuffle(data_csv)\n",
    "splitting_index = int(np.floor(0.9 * len(data_csv)))\n",
    "train_data = data_csv[:splitting_index]\n",
    "eval_data = data_csv[splitting_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export data\n",
    "with open('./my_train_data.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for row in train_data:\n",
    "        writer.writerow(row)\n",
    "        \n",
    "with open('./my_eval_data.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for row in eval_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a `classes.txt` file\n",
    "This file contains the color codes for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_colors = [[0, 0, 0],  # black\n",
    "               [255, 0, 0]] # red\n",
    "\n",
    "with open('./classes.txt', 'w') as f:\n",
    "    for color in list_colors:\n",
    "        [f.write(str(val) + ' ') for val in color]\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a config file\n",
    "This file contains all the parameters that are needed during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"classes_file\": os.path.join(dhd_wk_dir, 'classes.txt'),\n",
    "  \"eval_data\": os.path.join(dhd_wk_dir, 'my_eval_data.csv'),\n",
    "  \"train_data\": os.path.join(dhd_wk_dir, 'my_train_data.csv'),\n",
    "  \"model_output_dir\": os.path.join(dhd_wk_dir, 'exported-model'),\n",
    "  \"prediction_type\": \"CLASSIFICATION\",\n",
    "  \"pretrained_model_name\": \"resnet50\",\n",
    "  \"training_params\": {\n",
    "    \"batch_size\": 16,\n",
    "    \"data_augmentation\": True,\n",
    "    \"data_augmentation_color\": True,\n",
    "    \"data_augmentation_flip_lr\": True,\n",
    "    \"data_augmentation_flip_ud\": True,\n",
    "    \"data_augmentation_max_rotation\": 0.2,\n",
    "    \"data_augmentation_max_scaling\": 0.3,\n",
    "    \"input_resized_size\": 900000,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"make_patches\": True,\n",
    "    \"n_epochs\": 20,\n",
    "    \"patch_shape\": [\n",
    "      300,\n",
    "      300\n",
    "    ],\n",
    "    \"training_margin\": 10\n",
    "  }\n",
    "}\n",
    "\n",
    "assert os.path.isfile(config['classes_file'])\n",
    "assert os.path.isfile(config['eval_data'])\n",
    "assert os.path.isfile(config['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train\n",
    "In order to train the model you should \n",
    "1. clone and install dhSegment (intructions [here](https://dhsegment.readthedocs.io/en/latest/start/install.html))\n",
    "2. download the pretrained weights (step 2. detailed [here](https://dhsegment.readthedocs.io/en/latest/start/demo.html))\n",
    "3. run `python train with <your-path>/config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"<your-path> is {}\".format(dhd_wk_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from dh_segment.inference import LoadedModel\n",
    "import pandas as pd\n",
    "\n",
    "from dh_segment.post_processing import binarization, boxes_detection\n",
    "from dh_segment.io import PAGE\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_process_probs_ornament(probability_maps):\n",
    "\n",
    "    binary_maps = np.zeros_like(probability_maps, np.uint8)\n",
    "    binary_maps = np.delete(binary_maps, 0, 2)\n",
    "\n",
    "    # Ornament\n",
    "    binary_image = binarization.thresholding(probability_maps[:, :, 1], threshold=0.5)\n",
    "    binary_image = binarization.cleaning_binary(binary_image, kernel_size=7)\n",
    "    boxes = boxes_detection.find_boxes(binary_image, mode='rectangle', min_area=0.)\n",
    "    bin_map = np.zeros_like(binary_maps)\n",
    "    binary_maps[:, :, 0] = cv2.fillPoly(bin_map, boxes, (255, 0, 0))[:, :, 0]\n",
    "\n",
    "    return binary_maps, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_image_coordinates(input_coordinates, input_shape, resized_shape):\n",
    "    \n",
    "    rx = input_shape[0] / resized_shape[0]\n",
    "    ry = input_shape[1] / resized_shape[1]\n",
    "\n",
    "    return np.stack((np.round(input_coordinates[:, 0] / ry),\n",
    "                      np.round(input_coordinates[:, 1] / rx)), axis=1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start a TensorFlow Session with 1 GPU device\n",
    "session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list=\"0\"))\n",
    "session = tf.InteractiveSession(config=session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = './model_ornaments/'\n",
    "\n",
    "# Load model \n",
    "model = LoadedModel(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = './images/test_images/'\n",
    "filenames_to_process = glob(test_images_dir + '*.jpg')\n",
    "filenames_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example with one image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_filename = filenames_to_process[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(example_filename)\n",
    "probability_map = predictions['probs'][0]\n",
    "original_image_shape = predictions['original_shape'] # [H,W] shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post-process predictions\n",
    "binary_map, boxes = post_process_probs_ornament(predictions['probs'][0])\n",
    "boxes_resized = [resize_image_coordinates(box, \n",
    "                                          probability_map.shape[:2], \n",
    "                                          original_image_shape) for box in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(imread(example_filename))\n",
    "plt.title('original image')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(probability_map[:,:,1], cmap='gray')\n",
    "plt.title('probability map')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(binary_map[:,:,0], cmap='gray')\n",
    "plt.title('binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export all images detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data_export_dir = os.path.join(test_images_dir, 'page')\n",
    "os.makedirs(new_data_export_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in filenames_to_process:\n",
    "    predictions = model.predict(filename)\n",
    "    probability_map = predictions['probs'][0]\n",
    "    original_image_shape = predictions['original_shape'] # [H,W] shape\n",
    "    \n",
    "    # post-rpocess\n",
    "    binary_map, boxes = post_process_probs_ornament(predictions['probs'][0])\n",
    "    boxes_resized = [resize_image_coordinates(box, \n",
    "                                              probability_map.shape[:2], \n",
    "                                              original_image_shape) for box in boxes]\n",
    "    \n",
    "    # export\n",
    "    text_regions = [PAGE.TextRegion(id='txt-reg-{}'.format(i), \n",
    "                                coords=PAGE.Point.array_to_point(coords), \n",
    "                                custom_attribute=\"structure{type:drop-cap;}\") for i, coords in enumerate(boxes_resized)]\n",
    "    \n",
    "    page = PAGE.Page(image_filename=os.path.basename(filename),\n",
    "                     image_height=original_image_shape[0],\n",
    "                     image_width=original_image_shape[1],\n",
    "                     text_regions=text_regions)\n",
    "\n",
    "    page.write_to_file(filename=os.path.join(new_data_export_dir, \n",
    "                                             os.path.basename(filename).split('.')[0] + '.xml'),\n",
    "                       creator_name='OrnamentExtractor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all the eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get eval data\n",
    "df_eval = pd.read_csv('./my_eval_data.csv', header=None, names=['images', 'labels'])\n",
    "filenames_eval = list(df_eval.images.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_export_dir = './eval_page/'\n",
    "os.makedirs(eval_export_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames_eval:\n",
    "    predictions = model.predict(filename)\n",
    "    probability_map = predictions['probs'][0]\n",
    "    original_image_shape = predictions['original_shape'] # [H,W] shape\n",
    "    \n",
    "    # post-rpocess\n",
    "    binary_map, boxes = post_process_probs_ornament(predictions['probs'][0])\n",
    "    boxes_resized = [resize_image_coordinates(box, \n",
    "                                              probability_map.shape[:2], \n",
    "                                              original_image_shape) for box in boxes]\n",
    "    \n",
    "    # export\n",
    "    text_regions = [PAGE.TextRegion(id='txt-reg-{}'.format(i), \n",
    "                                coords=PAGE.Point.array_to_point(coords), \n",
    "                                custom_attribute=\"structure{type:drop-cap;}\") for i, coords in enumerate(boxes_resized)]\n",
    "    \n",
    "    page = PAGE.Page(image_filename=os.path.basename(filename),\n",
    "                     image_height=original_image_shape[0],\n",
    "                     image_width=original_image_shape[1],\n",
    "                     text_regions=text_regions)\n",
    "\n",
    "    page.write_to_file(filename=os.path.join(eval_export_dir, \n",
    "                                             os.path.basename(filename).split('.')[0] + '.xml'),\n",
    "                       creator_name='OrnamentExtractor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from dh_segment.io import PAGE\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_xml_dir = './images/page/'\n",
    "eval_xml_files = glob(eval_export_dir + '*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_iou = list()\n",
    "list_predicted_polygons = list()\n",
    "list_gt_polygons = list()\n",
    "for eval_filename in eval_xml_files:\n",
    "    predicted_page = PAGE.parse_file(eval_filename)\n",
    "    gt_page = PAGE.parse_file(os.path.join(gt_xml_dir, os.path.basename(eval_filename)))\n",
    "    \n",
    "    predicted_polygons = MultiPolygon([Polygon(PAGE.Point.point_to_list(region.coords)) \n",
    "                                   for region in predicted_page.text_regions])\n",
    "    gt_polygons = MultiPolygon([Polygon(PAGE.Point.point_to_list(region.coords)) \n",
    "                                for region in gt_page.text_regions])\n",
    "    \n",
    "    # Save polygons to visulise them later\n",
    "    list_predicted_polygons.append(predicted_polygons)\n",
    "    list_gt_polygons.append(gt_polygons)\n",
    "    \n",
    "    iou = predicted_polygons.intersection(gt_polygons).area / predicted_polygons.union(gt_polygons).area\n",
    "    \n",
    "    list_iou.append(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(list_iou[i])\n",
    "MultiPolygon([*list_predicted_polygons[i], *list_gt_polygons[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_segment",
   "language": "python",
   "name": "dh_segment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
